# 独自のgpt AIチャットボットを訓練と作成（自分の88本のQiita記事を使って、チャットAIを訓練してみた）

[Qiita](https://qiita.com/MaShunzhe/items/1117ced6000bd6b43715)

この記事では、独自のGPTチャットAIをゼロからトレーニングするプロセスについて説明します。

- 注：この記事の焦点は、既存のAIモジュールを微調整(Fine-Tuning)することではなく、あなたの入力テキストから会話スキルを学習するAIを訓練することです。
- OpenAIは多くのリソースと新しい技術を投入しているため、ChatGPTに近いものを訓練することを期待すべきではありません。（笑

十分なデータ（例：Wikipediaのすべてのコンテンツ）があれば、GPT-2に似たモデルをトレーニングすることが可能です。

https://openai.com/research/better-language-models

本記事では、日本語で書かれた **自分の88本のQiita記事** をデータソースとしてAIをトレーニングします（笑
このデータセットは非常に限られており、結果は最適ではありませんのでご注意ください。

この記事では、AIに関する経験がなくても手順に従って進められるように、段階的に説明します。

## 最終結果

### 88のQiita記事だけを使って **1万回(Mac miniで約1時間）** のトレーニングを繰り返した後：

<img width="958" alt="68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f37346462626439622d323933362d326538312d366261622d6639303866663031366134642e706e67" src="https://user-images.githubusercontent.com/68307970/232361955-6fe17eea-6337-4896-895c-bd1cf8acc8bd.png">
